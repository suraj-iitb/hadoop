# YARN: Yet Another Resource Negotiator (introduced in hadoop 2.0)

**Initial Idea**: Split resource management and job scheduling responsibilities of JobTracker

## Limitations before YARN:
1. Non MapReduce Jobs: real-time processing, graph processing
2. Scalability: JobTracker does majority of tasks (scheduling, monitoring, restart on failure for jobs). hence scalibilty limit is 4000 nodes and 40000 tasks.
3. High Availability: JobTracker is single point of failure
4. Memory Utilization: Pre-configured memory for map and reduce tasks

## YARN is built on master/slave architecture. Its components are:
- Resource Manager (Master): 1 master node
  1. Scheduler: Allocate the required resources requested by App Master (per-application), Schedule jobs
  2. Application Manager: Manage (launch, destroy) App Master (per-application)
- Node Manager (Slave): All slave node
  1. Launching & executing containers
  2. Sends heartbeats to Resource Manager
- Application Master (Per-application): 1 slave node
  1. Requeste required resources
  2. Manage (launch, destroy) application execution in cluster
  3. Sends heartbeats to Resource Manager
  
## How YARN works:
  1. Submit a job to YARN
  2. Resource Manager launches App Master for that job on one of the Node Manager
  3. App Master requests required resources from Resource Manager
  4. App Master coordinates with Node Manager to launch the container to execute the task



